{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import random\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from tensorflow.keras.layers import Input, Conv1D, GRU, TimeDistributed, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from td_utils import graph_spectrogram, load_raw_audio, match_target_amplitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/activates/1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/negatives/4.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/backgrounds/1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"audio_examples/example_train.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = graph_spectrogram(\"audio_examples/example_train.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, data = wavfile.read(\"audio_examples/example_train.wav\")\n",
    "print(\"Time steps in audio recording before spectrogram\", data[:,0].shape)\n",
    "print(\"Time steps in input after spectrogram\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 5511 # The number of time steps input to the model from the spectrogram\n",
    "n_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ty = 1375 # The number of time steps in the output of our model\n",
    "chime_file = \"data/audio_examples/chime.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio segments using pydub \n",
    "activates, negatives, backgrounds = load_raw_audio('./raw_data/')\n",
    "\n",
    "print(\"background len should be 10,000, since it is a 10 sec clip\\n\" + str(len(backgrounds[0])),\"\\n\")\n",
    "print(\"activate[0] len may be around 1000, since an `activate` audio clip is usually around 1 second (but varies a lot) \\n\" + str(len(activates[0])),\"\\n\")\n",
    "print(\"activate[1] len: different `activate` clips can have different lengths\\n\" + str(len(activates[1])),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_time_segment(segment_ms):\n",
    "    \"\"\"\n",
    "    Gets a random time segment of duration segment_ms in a 10,000 ms audio clip.\n",
    "    \n",
    "    Arguments:\n",
    "    segment_ms -- the duration of the audio clip in ms (\"ms\" stands for \"milliseconds\")\n",
    "    \n",
    "    Returns:\n",
    "    segment_time -- a tuple of (segment_start, segment_end) in ms\n",
    "    \"\"\"\n",
    "    \n",
    "    segment_start = np.random.randint(low=0, high=10000-segment_ms)   \n",
    "    segment_end = segment_start + segment_ms - 1\n",
    "    \n",
    "    return (segment_start, segment_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "def load_model():\n",
    "    json_path = 'models/model.json'\n",
    "    weights_path = 'models/model.h5'\n",
    "    with open(json_path, 'r') as file:\n",
    "        model = model_from_json(file.read())\n",
    "    model.load_weights(weights_path)\n",
    "    return model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate spectrogram and predict trigger word\n",
    "def detect_triggerword(filename):\n",
    "    x = graph_spectrogram(filename)\n",
    "    x = np.expand_dims(x.swapaxes(0, 1), axis=0)\n",
    "    predictions = model.predict(x)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "audio_clip, segment_time = insert_audio_clip(backgrounds[0], activates[0], [(3790, 4400)])\n",
    "audio_clip.export(\"insert_test.wav\", format=\"wav\")\n",
    "print(\"Segment Time: \", segment_time)\n",
    "IPython.display.Audio(\"insert_test.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected audio\n",
    "IPython.display.Audio(\"audio_examples/insert_reference.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay chime at detected positions\n",
    "def chime_on_activate(filename, predictions, threshold=0.5):\n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    chime = AudioSegment.from_wav(chime_file)\n",
    "    Ty = predictions.shape[1]\n",
    "    consecutive_timesteps = 0\n",
    "    for i in range(Ty):\n",
    "        consecutive_timesteps += 1\n",
    "        if predictions[0, i, 0] > threshold and consecutive_timesteps > 20:\n",
    "            audio_clip = audio_clip.overlay(chime, position=(i / Ty) * audio_clip.duration_seconds * 1000)\n",
    "            consecutive_timesteps = 0\n",
    "        elif predictions[0, i, 0] < threshold:\n",
    "            consecutive_timesteps = 0\n",
    "    output_path = \"output/chime_output.wav\"\n",
    "    audio_clip.export(output_path, format='wav')\n",
    "    print(f\"Chime output saved as {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to preprocess custom audio for compatibility\n",
    "def preprocess_audio(filename):\n",
    "    padding = AudioSegment.silent(duration=10000)\n",
    "    segment = AudioSegment.from_wav(filename)[:10000]\n",
    "    segment = padding.overlay(segment).set_frame_rate(44100)\n",
    "    segment.export(filename, format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the detection and chime\n",
    "def run_detection(filename, threshold=0.5):\n",
    "    preprocess_audio(filename)\n",
    "    predictions = detect_triggerword(filename)\n",
    "    chime_on_activate(filename, predictions, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(18)\n",
    "x, y = create_training_example(backgrounds[0], activates, negatives, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"train.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"audio_examples/train_reference.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed dev set examples\n",
    "X_dev = np.load(\"./XY_dev/X_dev.npy\")\n",
    "Y_dev = np.load(\"./XY_dev/Y_dev.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from test_utils import *\n",
    "\n",
    "def modelf_test(target):\n",
    "    Tx = 5511\n",
    "    n_freq = 101\n",
    "    model = target(input_shape = (Tx, n_freq))\n",
    "    expected_model = [['InputLayer', [(None, 5511, 101)], 0],\n",
    "                     ['Conv1D', (None, 1375, 196), 297136, 'valid', 'linear', (4,), (15,), 'GlorotUniform'],\n",
    "                     ['BatchNormalization', (None, 1375, 196), 784],\n",
    "                     ['Activation', (None, 1375, 196), 0],\n",
    "                     ['Dropout', (None, 1375, 196), 0, 0.8],\n",
    "                     ['GRU', (None, 1375, 128), 125184, True],\n",
    "                     ['Dropout', (None, 1375, 128), 0, 0.8],\n",
    "                     ['BatchNormalization', (None, 1375, 128), 512],\n",
    "                     ['GRU', (None, 1375, 128), 99072, True],\n",
    "                     ['Dropout', (None, 1375, 128), 0, 0.8],\n",
    "                     ['BatchNormalization', (None, 1375, 128), 512],\n",
    "                     ['Dropout', (None, 1375, 128), 0, 0.8],\n",
    "                     ['TimeDistributed', (None, 1375, 1), 129, 'sigmoid']]\n",
    "    comparator(summary(model), expected_model)\n",
    "    \n",
    "    \n",
    "modelf_test(modelf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelf(input_shape = (Tx, n_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=1e-6, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, batch_size = 16, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, = model.evaluate(X_dev, Y_dev)\n",
    "print(\"Dev set accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/dev/1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/dev/2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./raw_data/dev/1.wav\"\n",
    "prediction = detect_triggerword(filename)\n",
    "chime_on_activate(filename, prediction, 0.5)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename  = \"./raw_data/dev/2.wav\"\n",
    "prediction = detect_triggerword(filename)\n",
    "chime_on_activate(filename, prediction, 0.5)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_filename = \"audio_examples/my_audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_audio(your_filename)\n",
    "IPython.display.Audio(your_filename) # listen to the audio you uploaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chime_threshold = 0.5\n",
    "prediction = detect_triggerword(your_filename)\n",
    "chime_on_activate(your_filename, prediction, chime_threshold)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
